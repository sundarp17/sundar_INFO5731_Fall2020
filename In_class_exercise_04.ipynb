{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "In_class_exercise_04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundarp17/sundar_info5731_fall2020/blob/master/In_class_exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EuX00KHNeSpw"
      },
      "source": [
        "# **The fourth in-class-exercise (20 points in total)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s-vTOb03hG1f"
      },
      "source": [
        "# 1. Text Data Preprocessing\n",
        "\n",
        "Here is a [legal case](https://github.com/unt-iialab/INFO5731_FALL2020/blob/master/In_class_exercise/01-05-1%20%20Adams%20v%20Tanner.txt) we collected from westlaw, please follow the steps we mentioned in lesson 5 to clean the data:\n",
        "\n",
        "\n",
        "\n",
        "## 1.1 Basic feature extraction using text data (4 points)\n",
        "\n",
        "*   Number of sentences\n",
        "*   Number of words\n",
        "*   Number of characters\n",
        "*   Average word length\n",
        "*   Number of stopwords\n",
        "*   Number of special characters\n",
        "*   Number of numerics\n",
        "*   Number of uppercase words\n",
        "\n",
        "## 1.2 Basic Text Pre-processing of text data (4 points)\n",
        "\n",
        "*   Lower casing\n",
        "*   Punctuation removal\n",
        "*   Stopwords removal\n",
        "*   Frequent words removal\n",
        "*   Rare words removal\n",
        "*   Spelling correction\n",
        "*   Tokenization\n",
        "*   Stemming\n",
        "*   Lemmatization\n",
        "\n",
        "## 1.3 Save all the **clean sentences** to a **csv file** (one column, each raw is a sentence) after finishing all the steps above. (4 points)\n",
        "\n",
        "\n",
        "## 1.4 Advance Text Processing (Extra credit: 4 points)\n",
        "\n",
        "*   Calculate the term frequency of all the terms.\n",
        "*   Print out top 10 1-gram, top 10 2-grams, and top 10 3-grams terms as features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vR0L3_CreM_A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "24214893-2790-4186-e217-1f8fe1ccb6f5"
      },
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "words=stopwords.words('english')\n",
        "f=open('/content/Adam_tanner.txt',\"r\")\n",
        "text=f.read()\n",
        "\n",
        "def avg(text):\n",
        "  total=[]\n",
        "  for w in (text.split(\" \")):\n",
        "    total.append(len(w))\n",
        "  return sum(total)/len(text.split(\" \"))\n",
        "\n",
        "def stop(text):\n",
        "  total=0\n",
        "  for w in (text.split(\" \")):\n",
        "    if w in words:\n",
        "      total+=1\n",
        "  return total\n",
        "\n",
        "def special(text):\n",
        "  total=0\n",
        "  for w in text:\n",
        "       total += sum(not x.isalnum() for x in w)\n",
        "  return total\n",
        "\n",
        "def numerics(text):\n",
        "  total=0\n",
        "  for w in text.split():\n",
        "    total+=sum(x.isdigit() for x in w)\n",
        "  return total\n",
        "\n",
        "def upper_ch(text):\n",
        "  total=0\n",
        "  for w in text.split(\" \"):\n",
        "    total+=sum(x.isupper() for x in w)\n",
        "  return total\n",
        "\n",
        "def feature_extraction(text):\n",
        "  print(\"Number of sentences :\", text.count('.'))\n",
        "  print(\"Number of words :\",len(text.split(\" \")))\n",
        "  print(\"Number of characters :\",len(text))\n",
        "  print(\"Average word length\",avg(text))\n",
        "  print(\"Number of stopwords:\",stop(text))\n",
        "  print(\"Number of special characters :\", special(text))\n",
        "  print(\"Number of numerics :\", numerics(text))\n",
        "  print(\"Number of uppercase words: \",upper_ch(text))\n",
        "\n",
        "\n",
        "feature_extraction(text)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Number of sentences : 291\n",
            "Number of words : 3576\n",
            "Number of characters : 20452\n",
            "Average word length 4.719519015659955\n",
            "Number of stopwords: 1671\n",
            "Number of special characters : 4549\n",
            "Number of numerics : 356\n",
            "Number of uppercase words:  695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3UVPkULZ1G0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c77bd9d7-15e2-4364-cbd9-05061c63939d"
      },
      "source": [
        "#1.2\n",
        "import pandas as pd\n",
        "import string\n",
        "from textblob import Word\n",
        "from textblob import TextBlob\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "st=PorterStemmer()\n",
        "data=[]\n",
        "for l in text.split(\".\"):\n",
        "  nl=l.replace('\\n',' ')\n",
        "  data.append(nl)\n",
        "df=pd.DataFrame(data,columns=['original_text'])\n",
        "\n",
        "\n",
        "\n",
        "df['lower_case']=df['original_text'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
        "df['punctuation']=df['lower_case'].apply(lambda  x: \" \".join(x for x in x.split() if x not in string.punctuation))\n",
        "df['stop_words']=df['punctuation'].apply(lambda x:\" \".join(x for x in x.split() if x not in words))\n",
        "freq=pd.Series(\" \".join(df['stop_words']).split()).value_counts()[:10]\n",
        "print(freq)\n",
        "freq=list(freq.index)\n",
        "df['frequent']=df['stop_words'].apply(lambda x:\" \".join(x for x in x.split() if x not in freq))\n",
        "rare=pd.Series(\" \".join(df['frequent']).split()).value_counts()[-10:]\n",
        "print(rare)\n",
        "rare=list(rare.index)\n",
        "df['rare']=df['frequent'].apply(lambda x:\" \".join(x for x in x.split() if x not in rare))\n",
        "df['spelling']=df['rare'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "df['token']=df['spelling'].apply(lambda x: TextBlob(x).words)\n",
        "df['stemming']=df['token'].apply(lambda x: \" \".join([st.stem(word) for word in x]))\n",
        "df['lemmatization']=df['spelling'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "df.head(10)\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "execution    39\n",
            "crop         30\n",
            "levy         24\n",
            "v            22\n",
            "lien         21\n",
            "right        19\n",
            "court        18\n",
            "claimants    16\n",
            "contract     15\n",
            "rep          15\n",
            "dtype: int64\n",
            "principles     1\n",
            "ormond,        1\n",
            "existed        1\n",
            "injunction,    1\n",
            "satisfied,     1\n",
            "grow           1\n",
            "remove         1\n",
            "--1            1\n",
            "growth         1\n",
            "partner;       1\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_text</th>\n",
              "      <th>lower_case</th>\n",
              "      <th>punctuation</th>\n",
              "      <th>stop_words</th>\n",
              "      <th>frequent</th>\n",
              "      <th>rare</th>\n",
              "      <th>spelling</th>\n",
              "      <th>token</th>\n",
              "      <th>stemming</th>\n",
              "      <th>lemmatization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5 Ala</td>\n",
              "      <td>5 ala</td>\n",
              "      <td>5 ala</td>\n",
              "      <td>5 ala</td>\n",
              "      <td>5 ala</td>\n",
              "      <td>5 ala</td>\n",
              "      <td>5 all</td>\n",
              "      <td>[5, all]</td>\n",
              "      <td>5 all</td>\n",
              "      <td>5 all</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>740 Supreme Court of Alabama</td>\n",
              "      <td>740 supreme court of alabama</td>\n",
              "      <td>740 supreme court of alabama</td>\n",
              "      <td>740 supreme court alabama</td>\n",
              "      <td>740 supreme alabama</td>\n",
              "      <td>740 supreme alabama</td>\n",
              "      <td>740 supreme alabama</td>\n",
              "      <td>[740, supreme, alabama]</td>\n",
              "      <td>740 suprem alabama</td>\n",
              "      <td>740 supreme alabama</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ADAMS v</td>\n",
              "      <td>adams v</td>\n",
              "      <td>adams v</td>\n",
              "      <td>adams v</td>\n",
              "      <td>adams</td>\n",
              "      <td>adams</td>\n",
              "      <td>adams</td>\n",
              "      <td>[adams]</td>\n",
              "      <td>adam</td>\n",
              "      <td>adam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TANNER AND HORTON</td>\n",
              "      <td>tanner and horton</td>\n",
              "      <td>tanner and horton</td>\n",
              "      <td>tanner horton</td>\n",
              "      <td>tanner horton</td>\n",
              "      <td>tanner horton</td>\n",
              "      <td>manner norton</td>\n",
              "      <td>[manner, norton]</td>\n",
              "      <td>manner norton</td>\n",
              "      <td>manner norton</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>June Term, 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "      <td>[june, term, 1843]</td>\n",
              "      <td>june term 1843</td>\n",
              "      <td>june term, 1843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Synopsis WRIT of Error to the Circuit Court o...</td>\n",
              "      <td>synopsis writ of error to the circuit court of...</td>\n",
              "      <td>synopsis writ of error to the circuit court of...</td>\n",
              "      <td>synopsis writ error circuit court sumter</td>\n",
              "      <td>synopsis writ error circuit sumter</td>\n",
              "      <td>synopsis writ error circuit sumter</td>\n",
              "      <td>sycosis writ error circuit sumter</td>\n",
              "      <td>[sycosis, writ, error, circuit, sumter]</td>\n",
              "      <td>sycosi writ error circuit sumter</td>\n",
              "      <td>sycosis writ error circuit sumter</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>West Headnotes (2)   [1] Chattel Mortgage...</td>\n",
              "      <td>west headnotes (2) [1] chattel mortgages crops...</td>\n",
              "      <td>west headnotes (2) [1] chattel mortgages crops...</td>\n",
              "      <td>west headnotes (2) [1] chattel mortgages crops...</td>\n",
              "      <td>west headnotes (2) [1] chattel mortgages crops...</td>\n",
              "      <td>west headnotes (2) [1] chattel mortgages crops...</td>\n",
              "      <td>west headnotes (2) [1] chattel mortgages crops...</td>\n",
              "      <td>[west, headnotes, 2, 1, chattel, mortgages, cr...</td>\n",
              "      <td>west headnot 2 1 chattel mortgag crop grow exi...</td>\n",
              "      <td>west headnotes (2) [1] chattel mortgage crop g...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4 Cases that cite this headnote  [2] Creditor...</td>\n",
              "      <td>4 cases that cite this headnote [2] creditors’...</td>\n",
              "      <td>4 cases that cite this headnote [2] creditors’...</td>\n",
              "      <td>4 cases cite headnote [2] creditors’ remedies ...</td>\n",
              "      <td>4 cases cite headnote [2] creditors’ remedies ...</td>\n",
              "      <td>4 cases cite headnote [2] creditors’ remedies ...</td>\n",
              "      <td>4 cases cite headnote [2] creditors’ remedies ...</td>\n",
              "      <td>[4, cases, cite, headnote, 2, creditors, ’, re...</td>\n",
              "      <td>4 case cite headnot 2 creditor ’ remedi priori...</td>\n",
              "      <td>4 case cite headnote [2] creditors’ remedy pri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1821, prohibiting a levy on a crop until it ha...</td>\n",
              "      <td>1821, prohibiting a levy on a crop until it ha...</td>\n",
              "      <td>1821, prohibiting a levy on a crop until it ha...</td>\n",
              "      <td>1821, prohibiting levy crop gathered, lien att...</td>\n",
              "      <td>1821, prohibiting gathered, attaches favor fi</td>\n",
              "      <td>1821, prohibiting gathered, attaches favor fi</td>\n",
              "      <td>1821, prohibiting gathered, attaches favor i</td>\n",
              "      <td>[1821, prohibiting, gathered, attaches, favor, i]</td>\n",
              "      <td>1821 prohibit gather attach favor i</td>\n",
              "      <td>1821, prohibiting gathered, attache favor i</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>fa</td>\n",
              "      <td>fa</td>\n",
              "      <td>fa</td>\n",
              "      <td>fa</td>\n",
              "      <td>fa</td>\n",
              "      <td>fa</td>\n",
              "      <td>a</td>\n",
              "      <td>[a]</td>\n",
              "      <td>a</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       original_text  ...                                      lemmatization\n",
              "0                                              5 Ala  ...                                              5 all\n",
              "1                       740 Supreme Court of Alabama  ...                                740 supreme alabama\n",
              "2                                            ADAMS v  ...                                               adam\n",
              "3                                  TANNER AND HORTON  ...                                      manner norton\n",
              "4                                    June Term, 1843  ...                                    june term, 1843\n",
              "5   Synopsis WRIT of Error to the Circuit Court o...  ...                  sycosis writ error circuit sumter\n",
              "6       West Headnotes (2)   [1] Chattel Mortgage...  ...  west headnotes (2) [1] chattel mortgage crop g...\n",
              "7   4 Cases that cite this headnote  [2] Creditor...  ...  4 case cite headnote [2] creditors’ remedy pri...\n",
              "8  1821, prohibiting a levy on a crop until it ha...  ...        1821, prohibiting gathered, attache favor i\n",
              "9                                                 fa  ...                                                  a\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caSqhenInkIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv('pre_processing.csv',index=False)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BBiC4E_kefvV"
      },
      "source": [
        "# 2. Python Regular Expression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z1QJ-UwCenvN"
      },
      "source": [
        "## 2.1 Write a Python program to remove leading zeros from an IP address. (4 points)\n",
        "\n",
        "ip = \"260.08.094.109\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wSv6fVhOfFmv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "be38bd83-82f6-4db9-fe09-34614d4159c1"
      },
      "source": [
        "# Write your code here\n",
        "import re\n",
        "ip =  \"260.08.094.109\"\n",
        "zero = re.sub('\\.[0]*', '.', ip)\n",
        "zero\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'260.8.94.109'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXRjaHzrfKAy"
      },
      "source": [
        "## 2.2 Write a Python Program to extract all the years from the following sentence. (4 points)\n",
        "\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7xdJpDx9gjbX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "86b47baa-7b75-485b-cc3f-2047d7fced88"
      },
      "source": [
        "# Write your code here\n",
        "import re\n",
        "sentence = \"The 2010s were a dramatic decade, filled with ups and downs, more than 1000 stroies have happened. As the decade comes to a close, Insider took a look back at some of the biggest headline-grabbing stories, from 2010 to 2019. The result was 119 news stories that ranged from the heartwarming rescue of a Thai boys' soccer team from a flooded cave to the divisive election of President Donald Trump.\"\n",
        "\n",
        "years=re.findall(r'2\\d{3}', sentence) \n",
        "years"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2010', '2010', '2019']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    }
  ]
}