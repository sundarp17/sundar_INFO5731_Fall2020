{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_three.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundarp17/sundar_info5731_fall2020/blob/master/Assignment_three.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Three**\n",
        "\n",
        "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Understand N-gram**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
        "\n",
        "(1) Count the frequency of all the N-grams (N=3).\n",
        "\n",
        "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
        "\n",
        "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFPKhC0m1fd",
        "outputId": "57984410-5848-461b-d1a0-604465d5fcf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "df=pd.read_csv('/content/joker_reviews.csv')\n",
        "df.head()\n",
        "\n"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User Name</th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MihaVrhunc</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lesterarnoldpinto</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aman_Goyal</td>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logical_guy</td>\n",
              "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kdagoulis26</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           User Name                                             Review\n",
              "0         MihaVrhunc  Every once in a while a movie comes, that trul...\n",
              "1  lesterarnoldpinto  This is a movie that only those who have felt ...\n",
              "2         Aman_Goyal  Truly a masterpiece, The Best Hollywood film o...\n",
              "3        logical_guy  Joaquin Phoenix gives a tour de force performa...\n",
              "4        kdagoulis26  Most of the time movies are anticipated like t..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg9DWqeBhdTO",
        "outputId": "1c375e77-55a6-4443-a844-987531a2b47a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "#cleaning the data\n",
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "sw=stopwords.words('english')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from textblob import Word\n",
        "from textblob import TextBlob\n",
        "from nltk.stem import PorterStemmer\n",
        "st=PorterStemmer()\n",
        "\n"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBwfnQmDq7II",
        "outputId": "7e4211f1-0bfa-4608-ae69-e706d5df4c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "source": [
        "df['lower_case']=df['Review'].apply(lambda x:\" \".join(x.lower() for x in x.split()))\n",
        "df['punctuation']=df['lower_case'].apply(lambda  x: \" \".join(x for x in x.split() if x not in string.punctuation))\n",
        "df['special_characters']=df['punctuation'].apply(lambda x:\" \".join(x.replace('[#,$,%,@,&]', '') for x in x.split()))\n",
        "df['stop_words']=df['special_characters'].apply(lambda x:\" \".join(x for x in x.split() if x not in words))\n",
        "df['numbers']=df['stop_words'].apply(lambda x:\" \".join(x.replace('\\d+', '') for x in x.split()))\n",
        "df['token']=df['numbers'].apply(lambda x: TextBlob(x).words)\n",
        "df['stemming']=df['token'].apply(lambda x: \" \".join([st.stem(word) for word in x]))\n",
        "df['lemmatization']=df['stemming'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>User Name</th>\n",
              "      <th>Review</th>\n",
              "      <th>lower_case</th>\n",
              "      <th>punctuation</th>\n",
              "      <th>special_characters</th>\n",
              "      <th>stop_words</th>\n",
              "      <th>numbers</th>\n",
              "      <th>token</th>\n",
              "      <th>stemming</th>\n",
              "      <th>lemmatization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MihaVrhunc</td>\n",
              "      <td>Every once in a while a movie comes, that trul...</td>\n",
              "      <td>every once in a while a movie comes, that trul...</td>\n",
              "      <td>every once in a while a movie comes, that trul...</td>\n",
              "      <td>every once in a while a movie comes, that trul...</td>\n",
              "      <td>every once in a while a movie comes, that trul...</td>\n",
              "      <td>every once in a while a movie comes, that trul...</td>\n",
              "      <td>[every, once, in, a, while, a, movie, comes, t...</td>\n",
              "      <td>everi onc in a while a movi come that truli ma...</td>\n",
              "      <td>everi onc in a while a movi come that truli ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lesterarnoldpinto</td>\n",
              "      <td>This is a movie that only those who have felt ...</td>\n",
              "      <td>this is a movie that only those who have felt ...</td>\n",
              "      <td>this is a movie that only those who have felt ...</td>\n",
              "      <td>this is a movie that only those who have felt ...</td>\n",
              "      <td>this is a movie that only those who have felt ...</td>\n",
              "      <td>this is a movie that only those who have felt ...</td>\n",
              "      <td>[this, is, a, movie, that, only, those, who, h...</td>\n",
              "      <td>thi is a movi that onli those who have felt al...</td>\n",
              "      <td>thi is a movi that onli those who have felt al...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aman_Goyal</td>\n",
              "      <td>Truly a masterpiece, The Best Hollywood film o...</td>\n",
              "      <td>truly a masterpiece, the best hollywood film o...</td>\n",
              "      <td>truly a masterpiece, the best hollywood film o...</td>\n",
              "      <td>truly a masterpiece, the best hollywood film o...</td>\n",
              "      <td>truly a masterpiece, the best hollywood film o...</td>\n",
              "      <td>truly a masterpiece, the best hollywood film o...</td>\n",
              "      <td>[truly, a, masterpiece, the, best, hollywood, ...</td>\n",
              "      <td>truli a masterpiec the best hollywood film of ...</td>\n",
              "      <td>truli a masterpiec the best hollywood film of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>logical_guy</td>\n",
              "      <td>Joaquin Phoenix gives a tour de force performa...</td>\n",
              "      <td>joaquin phoenix gives a tour de force performa...</td>\n",
              "      <td>joaquin phoenix gives a tour de force performa...</td>\n",
              "      <td>joaquin phoenix gives a tour de force performa...</td>\n",
              "      <td>joaquin phoenix gives a tour de force performa...</td>\n",
              "      <td>joaquin phoenix gives a tour de force performa...</td>\n",
              "      <td>[joaquin, phoenix, gives, a, tour, de, force, ...</td>\n",
              "      <td>joaquin phoenix give a tour de forc perform fe...</td>\n",
              "      <td>joaquin phoenix give a tour de forc perform fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>kdagoulis26</td>\n",
              "      <td>Most of the time movies are anticipated like t...</td>\n",
              "      <td>most of the time movies are anticipated like t...</td>\n",
              "      <td>most of the time movies are anticipated like t...</td>\n",
              "      <td>most of the time movies are anticipated like t...</td>\n",
              "      <td>most of the time movies are anticipated like t...</td>\n",
              "      <td>most of the time movies are anticipated like t...</td>\n",
              "      <td>[most, of, the, time, movies, are, anticipated...</td>\n",
              "      <td>most of the time movi are anticip like thi the...</td>\n",
              "      <td>most of the time movi are anticip like thi the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           User Name  ...                                      lemmatization\n",
              "0         MihaVrhunc  ...  everi onc in a while a movi come that truli ma...\n",
              "1  lesterarnoldpinto  ...  thi is a movi that onli those who have felt al...\n",
              "2         Aman_Goyal  ...  truli a masterpiec the best hollywood film of ...\n",
              "3        logical_guy  ...  joaquin phoenix give a tour de forc perform fe...\n",
              "4        kdagoulis26  ...  most of the time movi are anticip like thi the...\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsrmt05QivL2",
        "outputId": "b536ac75-4e25-44f8-c2b7-a4bad1e805ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#trigrams\n",
        "from nltk.util import ngrams\n",
        "trigrams={}\n",
        "for i in df['lemmatization']:\n",
        "  words = TextBlob(i).ngrams(3)\n",
        "  length = len(words)\n",
        "  if length>0:\n",
        "    for j in words:\n",
        "      k = tuple(j)\n",
        "      if k in trigrams:\n",
        "        trigrams[k]+=1\n",
        "      else:\n",
        "        trigrams[k] = 1\n",
        "\n",
        "trigrams_10 =  {k: trigrams[k] for k in list(trigrams)[:10]}\n",
        "trigrams_10\n",
        "\n",
        "\n"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('a', 'movi', 'come'): 1,\n",
              " ('a', 'while', 'a'): 1,\n",
              " ('come', 'that', 'truli'): 1,\n",
              " ('everi', 'onc', 'in'): 1,\n",
              " ('in', 'a', 'while'): 1,\n",
              " ('movi', 'come', 'that'): 1,\n",
              " ('onc', 'in', 'a'): 1,\n",
              " ('that', 'truli', 'make'): 1,\n",
              " ('truli', 'make', 'an'): 1,\n",
              " ('while', 'a', 'movi'): 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdLMKOld7xKU",
        "outputId": "818a6ae5-19a4-4da0-d09d-597f660d2ad7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#bigram\n",
        "bigrams={}\n",
        "for i in df['lemmatization']:\n",
        "  words = TextBlob(i).ngrams(2)\n",
        "  length = len(words)\n",
        "  if length>0:\n",
        "    for j in words:\n",
        "      k = tuple(j)\n",
        "      if k in bigrams:\n",
        "        bigrams[k]+=1\n",
        "      else:\n",
        "        bigrams[k] = 1\n",
        "\n",
        "bigrams_10 =  {k: bigrams[k] for k in list(bigrams)[:10]}\n",
        "bigrams_10\n",
        "\n"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('a', 'movi'): 23,\n",
              " ('a', 'while'): 1,\n",
              " ('come', 'that'): 1,\n",
              " ('everi', 'onc'): 1,\n",
              " ('in', 'a'): 20,\n",
              " ('movi', 'come'): 1,\n",
              " ('onc', 'in'): 1,\n",
              " ('that', 'truli'): 1,\n",
              " ('truli', 'make'): 1,\n",
              " ('while', 'a'): 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xQgYWKr8ne2",
        "outputId": "6827dd43-183b-4566-df3c-ab73df03facb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#unigram\n",
        "unigrams={}\n",
        "for i in df['lemmatization']:\n",
        "  words = TextBlob(i).ngrams(1)\n",
        "  length = len(words)\n",
        "  if length>0:\n",
        "    for j in words:\n",
        "      k = tuple(j)\n",
        "      if k in unigrams:\n",
        "        unigrams[k]+=1\n",
        "      else:\n",
        "        unigrams[k] = 1\n",
        "\n",
        "unigrams_10 =  {k: unigrams[k] for k in list(unigrams)[:10]}\n",
        "unigrams_10"
      ],
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('a',): 593,\n",
              " ('come',): 14,\n",
              " ('everi',): 11,\n",
              " ('in',): 200,\n",
              " ('make',): 36,\n",
              " ('movi',): 280,\n",
              " ('onc',): 4,\n",
              " ('that',): 214,\n",
              " ('truli',): 10,\n",
              " ('while',): 16}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 212
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4emC_gh9HNY",
        "outputId": "9dafaa69-7322-4227-88ae-992241396da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "#probability\n",
        "pb={}\n",
        "for key,val in bigrams.items():\n",
        "  pb[key]=val/unigrams[tuple((key[0],))]\n",
        "\n",
        "pb_10 = {k: pb[k] for k in list(pb)[:10]}\n",
        "pb_10"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('a', 'movi'): 0.0387858347386172,\n",
              " ('a', 'while'): 0.0016863406408094434,\n",
              " ('come', 'that'): 0.07142857142857142,\n",
              " ('everi', 'onc'): 0.09090909090909091,\n",
              " ('in', 'a'): 0.1,\n",
              " ('movi', 'come'): 0.0035714285714285713,\n",
              " ('onc', 'in'): 0.25,\n",
              " ('that', 'truli'): 0.004672897196261682,\n",
              " ('truli', 'make'): 0.1,\n",
              " ('while', 'a'): 0.0625}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQrSUK-DB6OB",
        "outputId": "50c2c533-2340-4b59-b88d-6e70d7d9f2ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "#3\n",
        "#Extract all the noun phrases and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by \n",
        "#using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. \n",
        "#Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets)\n",
        "\n",
        "#noun phrases\n",
        "nltk.download('brown')\n",
        "np={}\n",
        "for i in df['lemmatization']:\n",
        "  words = TextBlob(i).noun_phrases\n",
        "  for j in words:\n",
        "    if j in np:\n",
        "      np[j]+=1\n",
        "    else:\n",
        "      np[j] = 1\n",
        "np_10 ={k: np[k] for k in list(np)[:10]}\n",
        "\n",
        "#max frequency\n",
        "\n",
        "mf = max({ i for i in np.values()})\n",
        "np_10\n",
        "print('maximum frequency',mf)"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "maximum frequency 57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewxDyOEAEDWN"
      },
      "source": [
        "#relative probability\n",
        "\n",
        "rp={}\n",
        "for key, val in np.items():\n",
        "  rp[key]=val/mf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04FH-LIfG5HB",
        "outputId": "95f9dddd-bc9a-4ba5-e6dc-e8e3574e9cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "df_nouns = pd.DataFrame(list(rp.items()),columns = ['nouns','rp']) \n",
        "df_nouns.T"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>1561</th>\n",
              "      <th>1562</th>\n",
              "      <th>1563</th>\n",
              "      <th>1564</th>\n",
              "      <th>1565</th>\n",
              "      <th>1566</th>\n",
              "      <th>1567</th>\n",
              "      <th>1568</th>\n",
              "      <th>1569</th>\n",
              "      <th>1570</th>\n",
              "      <th>1571</th>\n",
              "      <th>1572</th>\n",
              "      <th>1573</th>\n",
              "      <th>1574</th>\n",
              "      <th>1575</th>\n",
              "      <th>1576</th>\n",
              "      <th>1577</th>\n",
              "      <th>1578</th>\n",
              "      <th>1579</th>\n",
              "      <th>1580</th>\n",
              "      <th>1581</th>\n",
              "      <th>1582</th>\n",
              "      <th>1583</th>\n",
              "      <th>1584</th>\n",
              "      <th>1585</th>\n",
              "      <th>1586</th>\n",
              "      <th>1587</th>\n",
              "      <th>1588</th>\n",
              "      <th>1589</th>\n",
              "      <th>1590</th>\n",
              "      <th>1591</th>\n",
              "      <th>1592</th>\n",
              "      <th>1593</th>\n",
              "      <th>1594</th>\n",
              "      <th>1595</th>\n",
              "      <th>1596</th>\n",
              "      <th>1597</th>\n",
              "      <th>1598</th>\n",
              "      <th>1599</th>\n",
              "      <th>1600</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>nouns</th>\n",
              "      <td>everi onc</td>\n",
              "      <td>impact joaquin 's</td>\n",
              "      <td>'s brillianc grotesqu haunt</td>\n",
              "      <td>wo n't blink</td>\n",
              "      <td>serious funni moment emot rollercoast sometim</td>\n",
              "      <td>multipl emot popping-up</td>\n",
              "      <td>typic action-riddl</td>\n",
              "      <td>super-hero movi</td>\n",
              "      <td>proper psycholog thriller/drama</td>\n",
              "      <td>truli relat</td>\n",
              "      <td>thi movi</td>\n",
              "      <td>encourag violenc</td>\n",
              "      <td>truli thi movi</td>\n",
              "      <td>thi world</td>\n",
              "      <td>hollywood film</td>\n",
              "      <td>comic book</td>\n",
              "      <td>real ife remark</td>\n",
              "      <td>direct cinematographi music</td>\n",
              "      <td>underappreciated/unrecognized/bulli peopl</td>\n",
              "      <td>show class differ</td>\n",
              "      <td>talent rule</td>\n",
              "      <td>'s believ</td>\n",
              "      <td>multipl joker</td>\n",
              "      <td>bitter way</td>\n",
              "      <td>film show</td>\n",
              "      <td>peopl uncomfort peopl consid thi</td>\n",
              "      <td>perfect film</td>\n",
              "      <td>joaquin phoenix</td>\n",
              "      <td>emot depth</td>\n",
              "      <td>'s imposs</td>\n",
              "      <td>referenc heath ledger 's oscar-win</td>\n",
              "      <td>dark knight</td>\n",
              "      <td>wide consid</td>\n",
              "      <td>definit live-act portray</td>\n",
              "      <td>potenti exce</td>\n",
              "      <td>dark knight 's clown princ</td>\n",
              "      <td>time movi</td>\n",
              "      <td>short way</td>\n",
              "      <td>short joker</td>\n",
              "      <td>time i wa</td>\n",
              "      <td>...</td>\n",
              "      <td>just look</td>\n",
              "      <td>true form</td>\n",
              "      <td>absolut stun</td>\n",
              "      <td>i side</td>\n",
              "      <td>evil charact</td>\n",
              "      <td>honestli u</td>\n",
              "      <td>long time movi</td>\n",
              "      <td>thi deserv</td>\n",
              "      <td>big screen</td>\n",
              "      <td>batman fan</td>\n",
              "      <td>i particularli fond</td>\n",
              "      <td>thi movi wa truli</td>\n",
              "      <td>viscer satisfi crescendo i</td>\n",
              "      <td>joker 's pain wa</td>\n",
              "      <td>palpabl i</td>\n",
              "      <td>oscar pool card</td>\n",
              "      <td>now.p thi movi</td>\n",
              "      <td>super hero movi leav</td>\n",
              "      <td>i wa underwhelm</td>\n",
              "      <td>fact becaus hollywood</td>\n",
              "      <td>magic looong time</td>\n",
              "      <td>bo dy</td>\n",
              "      <td>darth vader 's childhood</td>\n",
              "      <td>joker 's relationship</td>\n",
              "      <td>hi mother</td>\n",
              "      <td>proper boogeyman</td>\n",
              "      <td>overpaid lalaland-schmock wo n't</td>\n",
              "      <td>stupid prequel-explains-it-all-train plea</td>\n",
              "      <td>solid filmmak joker</td>\n",
              "      <td>old roll</td>\n",
              "      <td>unrel scene cheesi taxi-driver-mo</td>\n",
              "      <td>ani impact massiv overact</td>\n",
              "      <td>subtl sinist joker moment</td>\n",
              "      <td>trailer rate</td>\n",
              "      <td>joke heath ledger i</td>\n",
              "      <td>act cinematographi music screenplay costum got...</td>\n",
              "      <td>10/10.oscar worthi performancebi</td>\n",
              "      <td>joaquin phoenix.amaz</td>\n",
              "      <td>todd phillip amaz cinematographi</td>\n",
              "      <td>lawrenc sher</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rp</th>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0526316</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0350877</td>\n",
              "      <td>0.210526</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.140351</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "      <td>0.0175439</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 1601 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0     ...          1600\n",
              "nouns  everi onc  ...  lawrenc sher\n",
              "rp     0.0175439  ...     0.0175439\n",
              "\n",
              "[2 rows x 1601 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Undersand TF-IDF and Document representation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(40 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
        "\n",
        "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
        "\n",
        "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vATjQNTY8buA",
        "outputId": "84d29108-6db7-4e92-a0f2-ae6c38a205fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Write your code here\n",
        "!pip install texthero\n",
        "import texthero as hero\n",
        "df['tfidf'] = hero.tfidf(df['lemmatization'])\n",
        "df[['lemmatization','tfidf']]\n",
        "\n",
        "\n"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: texthero in /usr/local/lib/python3.6/dist-packages (1.0.9)\n",
            "Requirement already satisfied: tqdm>=4.3 in /usr/local/lib/python3.6/dist-packages (from texthero) (4.41.1)\n",
            "Requirement already satisfied: nltk>=3.3 in /usr/local/lib/python3.6/dist-packages (from texthero) (3.5)\n",
            "Requirement already satisfied: matplotlib>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from texthero) (3.2.2)\n",
            "Requirement already satisfied: gensim>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from texthero) (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from texthero) (1.18.5)\n",
            "Requirement already satisfied: pandas>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from texthero) (1.1.3)\n",
            "Requirement already satisfied: wordcloud>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from texthero) (1.5.0)\n",
            "Requirement already satisfied: unidecode>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from texthero) (1.1.1)\n",
            "Requirement already satisfied: plotly>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from texthero) (4.4.1)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from texthero) (2.2.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.6/dist-packages (from texthero) (0.22.2.post1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk>=3.3->texthero) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from nltk>=3.3->texthero) (0.17.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk>=3.3->texthero) (2019.12.20)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->texthero) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->texthero) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->texthero) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.6.0->texthero) (3.0.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.6.0->texthero) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.6.0->texthero) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.0.2->texthero) (2018.9)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud>=1.5.0->texthero) (7.0.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly>=4.2.0->texthero) (1.3.3)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (1.1.3)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (3.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (50.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (0.4.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->texthero) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (2.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (3.3.1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lemmatization</th>\n",
              "      <th>tfidf</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>everi onc in a while a movi come that truli ma...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>thi is a movi that onli those who have felt al...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>truli a masterpiec the best hollywood film of ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>joaquin phoenix give a tour de forc perform fe...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>most of the time movi are anticip like thi the...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>disturb just got out of the movi my humor wa m...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>wow i honestli got ta tell you it 's one of th...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>i am not a batman fan nor am i particularli fo...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>i wa underwhelm to say it friendli but not sur...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>act cinematographi music screenplay costum got...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        lemmatization                                              tfidf\n",
              "0   everi onc in a while a movi come that truli ma...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "1   thi is a movi that onli those who have felt al...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "2   truli a masterpiec the best hollywood film of ...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "3   joaquin phoenix give a tour de forc perform fe...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "4   most of the time movi are anticip like thi the...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "..                                                ...                                                ...\n",
              "95  disturb just got out of the movi my humor wa m...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "96  wow i honestli got ta tell you it 's one of th...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "97  i am not a batman fan nor am i particularli fo...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "98  i wa underwhelm to say it friendli but not sur...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "99  act cinematographi music screenplay costum got...  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
              "\n",
              "[100 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpeMgiYpVapX",
        "outputId": "ac56cbfd-48c6-4255-c5c8-4349934dab8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "\n",
        "def clean_text(text):\n",
        "  text=\"\".join([word.lower() for word in text if word not in string.punctuation])\n",
        "  tokens = re.split('\\W+',text)\n",
        "  text = [st.stem(word) for word in tokens if word not in sw]\n",
        "  return text\n",
        "\n",
        "df['non numbers']= df['lemmatization'].apply(lambda x:\" \".join(x for x in x.split() if not x.isdigit()))\n",
        "df['number_string']=df['non numbers'].apply(lambda x:\" \".join(x.replace('\\d+', '') for x in x.split()))\n",
        "#print(df['number string'].head(1))\n",
        "tfidf_vect = TfidfVectorizer()\n",
        "\n",
        "X_tfidf = tfidf_vect.fit_transform(df['number_string'])\n",
        "\n",
        "X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
        "X_tfidf_df.columns =tfidf_vect.get_feature_names()\n",
        "X_tfidf_df.head()"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    everi onc in a while a movi come that truli ma...\n",
            "Name: number string, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>10grade</th>\n",
              "      <th>12i</th>\n",
              "      <th>2nd</th>\n",
              "      <th>76th</th>\n",
              "      <th>aaah</th>\n",
              "      <th>abil</th>\n",
              "      <th>abl</th>\n",
              "      <th>about</th>\n",
              "      <th>abov</th>\n",
              "      <th>absolut</th>\n",
              "      <th>absurd</th>\n",
              "      <th>abus</th>\n",
              "      <th>accept</th>\n",
              "      <th>acclaim</th>\n",
              "      <th>accolad</th>\n",
              "      <th>accompli</th>\n",
              "      <th>accomplish</th>\n",
              "      <th>account</th>\n",
              "      <th>accur</th>\n",
              "      <th>achiev</th>\n",
              "      <th>achiv</th>\n",
              "      <th>across</th>\n",
              "      <th>act</th>\n",
              "      <th>acting</th>\n",
              "      <th>action</th>\n",
              "      <th>activ</th>\n",
              "      <th>actor</th>\n",
              "      <th>actual</th>\n",
              "      <th>ad</th>\n",
              "      <th>adapt</th>\n",
              "      <th>add</th>\n",
              "      <th>addit</th>\n",
              "      <th>adult</th>\n",
              "      <th>advanc</th>\n",
              "      <th>adventur</th>\n",
              "      <th>aesthet</th>\n",
              "      <th>affect</th>\n",
              "      <th>after</th>\n",
              "      <th>afternoonso</th>\n",
              "      <th>...</th>\n",
              "      <th>wit</th>\n",
              "      <th>with</th>\n",
              "      <th>without</th>\n",
              "      <th>witti</th>\n",
              "      <th>wo</th>\n",
              "      <th>woman</th>\n",
              "      <th>wonder</th>\n",
              "      <th>word</th>\n",
              "      <th>work</th>\n",
              "      <th>workplac</th>\n",
              "      <th>world</th>\n",
              "      <th>wors</th>\n",
              "      <th>worst</th>\n",
              "      <th>worth</th>\n",
              "      <th>worthi</th>\n",
              "      <th>would</th>\n",
              "      <th>wow</th>\n",
              "      <th>write</th>\n",
              "      <th>writer</th>\n",
              "      <th>writh</th>\n",
              "      <th>written</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrote</th>\n",
              "      <th>wtf</th>\n",
              "      <th>ya</th>\n",
              "      <th>yawn</th>\n",
              "      <th>ye</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yellow</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yet</th>\n",
              "      <th>you</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>your</th>\n",
              "      <th>yourself</th>\n",
              "      <th>yr</th>\n",
              "      <th>zazi</th>\n",
              "      <th>zero</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.098235</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.178925</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.119477</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.061870</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.060935</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102747</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.126423</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.048335</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.048335</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.132845</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.160922</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.066422</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.097301</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.119240</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.123696</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 2212 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    10  10grade  12i  2nd  76th  aaah  ...  younger  your  yourself   yr  zazi  zero\n",
              "0  0.0      0.0  0.0  0.0   0.0   0.0  ...      0.0   0.0       0.0  0.0   0.0   0.0\n",
              "1  0.0      0.0  0.0  0.0   0.0   0.0  ...      0.0   0.0       0.0  0.0   0.0   0.0\n",
              "2  0.0      0.0  0.0  0.0   0.0   0.0  ...      0.0   0.0       0.0  0.0   0.0   0.0\n",
              "3  0.0      0.0  0.0  0.0   0.0   0.0  ...      0.0   0.0       0.0  0.0   0.0   0.0\n",
              "4  0.0      0.0  0.0  0.0   0.0   0.0  ...      0.0   0.0       0.0  0.0   0.0   0.0\n",
              "\n",
              "[5 rows x 2212 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KdWIXJeiZp7",
        "outputId": "a92b5abd-f7ca-48ee-f4e3-fe4e096a10ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#query\n",
        "query=df['number_string'].head(1)\n",
        "query"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    everi onc in a while a movi come that truli ma...\n",
              "Name: number_string, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asoE8d4hgLIw",
        "outputId": "33171768-68b4-40e0-a6d7-432912a3fe7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "#cosine\n",
        "import math\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "WORD = re.compile(r\"\\w+\")\n",
        "\n",
        "\n",
        "def get_cosine(vec1, vec2):\n",
        "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
        "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
        "\n",
        "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
        "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
        "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
        "\n",
        "    if not denominator:\n",
        "        return 0.0\n",
        "    else:\n",
        "        return float(numerator) / denominator\n",
        "\n",
        "\n",
        "def text_to_vector(text):\n",
        "    words = WORD.findall(text)\n",
        "    return Counter(words)\n",
        "\n",
        "text1=query\n",
        "cosine=[]\n",
        "for i in df['number_string']:\n",
        "  vector1 = text_to_vector(str(text1))\n",
        "  vector2 = text_to_vector(i)\n",
        "  cosine.append(get_cosine(vector1, vector2))\n",
        "\n",
        "df_cosine = pd.DataFrame(cosine,columns=['Cosine similarity'])\n",
        "df_cosine['text'] = df['number_string']\n",
        "df_cosine.head()"
      ],
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Cosine similarity</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.357295</td>\n",
              "      <td>everi onc in a while a movi come that truli ma...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.282166</td>\n",
              "      <td>thi is a movi that onli those who have felt al...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.227068</td>\n",
              "      <td>truli a masterpiec the best hollywood film of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.122637</td>\n",
              "      <td>joaquin phoenix give a tour de forc perform fe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.183793</td>\n",
              "      <td>most of the time movi are anticip like thi the...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Cosine similarity                                               text\n",
              "0           0.357295  everi onc in a while a movi come that truli ma...\n",
              "1           0.282166  thi is a movi that onli those who have felt al...\n",
              "2           0.227068  truli a masterpiec the best hollywood film of ...\n",
              "3           0.122637  joaquin phoenix give a tour de forc perform fe...\n",
              "4           0.183793  most of the time movi are anticip like thi the..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: Create your own training and evaluation data for sentiment analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfvMKJjIXS5G"
      },
      "source": [
        "# The GitHub link of your final csv file\n",
        "\n",
        "# Link: https://github.com/sundarp17/sundar_info5731_fall2020/blob/master/sentimental_analysis.csv"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}